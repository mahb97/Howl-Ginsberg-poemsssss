{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM/On0gGUgg1Q8kKLBQ+cT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Howl-Ginsberg-poemsssss/blob/main/Visionarytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Bn0a_ktSZK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LA4k1LdSJOb"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch==2.4.1 bitsandbytes==0.43.3 transformers==4.44.2 datasets==2.21.0 peft==0.12.0 trl==0.9.6 accelerate==0.34.2\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import torch, os\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # or \"TinyLlama/TinyLlama-1.1B-intermediate-step-955k-2T\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "data = load_dataset(\"json\", data_files={\"/content/ginsberg.jsonl\": \"train\"}, split=\"train\")\n",
        "\n",
        "def format_example(ex):\n",
        "    return {\"text\": ex[\"text\"].strip()}\n",
        "\n",
        "data = data.map(format_example, remove_columns=data.column_names)\n",
        "\n",
        "# Be crazy dumbsaint of the mind\n",
        "bnb_cfg = dict(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", **bnb_cfg)\n",
        "\n",
        "# call my daughter LoRA\n",
        "lora = LoraConfig(\n",
        "    r=16, lora_alpha=32, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Scribbled secret notebooks, and wild typewritten pages, for yr own joy\n",
        "cfg = SFTConfig(\n",
        "    output_dir=\"/content/tinyllama-ginsberg-lora\",\n",
        "    num_train_epochs=2,               # low start; poetry overfits quickly\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    bf16=True,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    packing=True,\n",
        "    max_seq_length=1024,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=lora,\n",
        "    formatting_func=lambda batch: [x[\"text\"] for x in batch],\n",
        "    args=cfg,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()  # saves LoRA adapters\n",
        "\n",
        "from peft import PeftModel\n",
        "base = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
        "merged = PeftModel.from_pretrained(base, \"/content/tinyllama-ginsberg-lora\").merge_and_unload()\n",
        "merged.save_pretrained(\"/content/tinyllama-ginsberg-merged\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"/content/tinyllama-ginsberg-merged\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "import torch\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
        "tok  = AutoTokenizer.from_pretrained(model_name)\n",
        "ft   = PeftModel.from_pretrained(base, \"/content/tinyllama-ginsberg-lora\")\n",
        "\n",
        "prompt = \"Write a poem about cobblestones and rain.\\n\\n\"\n",
        "ids = tok(prompt, return_tensors=\"pt\").to(ft.device)\n",
        "streamer = TextStreamer(tok)\n",
        "out = ft.generate(\n",
        "    **ids,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.1,\n",
        "    eos_token_id=tok.eos_token_id\n",
        ")\n",
        "print(tok.decode(out[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "HMhGlSu2TKF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}